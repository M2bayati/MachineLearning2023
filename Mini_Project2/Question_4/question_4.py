# -*- coding: utf-8 -*-
"""Question_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZbDLaMwYUMf9K2A0bYdw5M1RJsalIOll

**Question 4**

## **Import Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
from datetime import datetime

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import r2_score

import tensorflow as tf
from tensorflow import keras
from keras import preprocessing
from keras.models import Sequential
from keras.layers import Dense

import warnings
warnings.filterwarnings("ignore")

"""# Part 1"""

!pip install --upgrade --no-cache-dir gdown
!gdown 1BZCk7WAz9WHz6mcIl1Ru8BIL9Yk86Q1Q
# https://drive.google.com/file/d/1BZCk7WAz9WHz6mcIl1Ru8BIL9Yk86Q1Q/view?usp=sharing

# Load the dataset from the specified file path
df = pd.read_csv("/content/data.csv")

# Display the first few rows of the DataFrame
df.head()

"""## Information about the DataFrame"""

# Display information about the DataFrame
df.info()

"""## Display the number of data that are NaN for each column"""

df.isnull().sum()

# Remove rows with any null values
# df.dropna(inplace=True)

"""## Drop some columns from the DataFrame

"""

# Drop the country and street and statezip columns from the DataFrame
df = df.drop(['statezip', 'country', 'street'], axis=1)

# Display information about the DataFrame
df.info()

# Extract the "City" column
City = df["city"]

# Count the occurrences of each unique city
City.value_counts()

"""# Part 2

## Correlation matrix
"""

# Calculate the correlation between columns and 'price', then sort them in descending order
correlation_matrix = df.corr()['price'].sort_values(ascending=False)
correlation_matrix

# Plot a heatmap to visualize the correlation matrix
plt.figure(figsize=(20, 20))
sns.heatmap(df.corr(), cmap="RdYlGn")
plt.title("correlation matrix for house price", fontsize=30)
plt.show()

# Select columns with numerical data types
num = df.select_dtypes(exclude=['object']).columns
num

# Create a heatmap to visualize the correlation matrix of numerical columns
plt.figure(figsize=(15, 15))
sns.heatmap(df[num].corr(), annot=True, cmap='inferno', mask=np.triu(df[num].corr(), k=1))
plt.title("correlation matrix for house price", fontsize=30)

"""# Part 3

## Plot Distribution and sqft_living
"""

# Create a scatter plot of  against price
plt.figure(figsize=(10, 8))
plt.scatter(x='sqft_living', y='price', data=df)
plt.xlabel('sqft_living ', fontsize=18)
plt.ylabel('price', fontsize=18)
plt.title('sqft_living  vs. Price', fontsize=18)
plt.grid(alpha=0.4)
plt.show()

"""## Distribution of Price"""

plt.figure(figsize=(10, 8))

sns.distplot(df['price'], color="green").set_title('price Distribution')
plt.grid(alpha=0.4)

"""# Part 4"""

# Extract 'year', 'month', and 'day' from the 'date' column
df['year'] = pd.to_datetime(df['date']).dt.year
df['month'] = pd.to_datetime(df['date']).dt.month

# Show the DataFrame with the separate 'year' and 'month' columns
df = df[['year', 'month'] + [col for col in df.columns if col not in ['year', 'month']]]

# Drop the specified columns from the DataFrame
df = df.drop(['date'], axis=1)
df

# Drop year columns from the DataFrame
df = df.drop(['year'], axis=1)
df

"""## Convert descriptive data to numerical data"""

# List of specified categorical columns
dummy = ['city']

# Convert categorical columns to numerical using one-hot encoding
df2 = pd.get_dummies(df, columns=dummy, drop_first=True)

# Display the first few rows of the modified DataFrame
df2.head()

# Initialize LabelEncoder
l1 = LabelEncoder()

# Convert object-type columns to numerical using Label Encoding
for i in df2.columns:
    if df2[i].dtype == 'object':
        df2[i] = l1.fit_transform(df2[i])

df2

"""# Part 5"""

# Separate features (X) and output (Y) data
X = df2.drop(["price"], axis=1)  # features
y = df2["price"]                  # Output data

# Splitting the dataset into the Training set and Test set (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=93, shuffle=True)

# Display the dimensions of the training and testing sets
print(f'Dimensions of the training features: {X_train.shape}')
print(f'Dimensions of the training target: {y_train.shape}')
print(f'Dimensions of the testing features: {X_test.shape}')
print(f'Dimensions of the testing target: {y_test.shape}')

"""## Min_Max Scalar"""

# Initialize Min-Max Scaler
scaler_1 = MinMaxScaler()

# Normalize the training input data
X_train = scaler_1.fit_transform(X_train)

# Normalize the test input data
X_test = scaler_1.transform(X_test)

# Convert y_train and y_test type to DataFrame
y_train = pd.DataFrame(y_train)
y_test = pd.DataFrame(y_test)

scaler_2 = MinMaxScaler()

# Normalize outputs
y_train = scaler_2.fit_transform(y_train)
y_test = scaler_2.transform(y_test)

"""# Part 6

## MLP with 3 hidden layer
"""

model_3 = Sequential()

# Add the first hidden layer with 10 neurons and ReLU activation function
model_3.add(Dense(10, activation='relu', input_shape=(X_train.shape[1],)))

# Add the second hidden layer with 10 neurons and ReLU activation function
model_3.add(Dense(10, activation='relu'))

# Add the third hidden layer with 10 neurons and ReLU activation function
model_3.add(Dense(10, activation='relu'))

# Add an output layer with 1 neuron and linear activation function
model_3.add(Dense(1, activation='linear'))

model_3.summary()

"""## Fit model_3"""

model_3.compile(optimizer='adam', loss='mse')

# Split the data into training and validation sets
X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=93, shuffle=True)

history = model_3.fit(X_train1, y_train1, validation_data=(X_val, y_val), epochs=100, batch_size=10, verbose=0)

# Evaluate the model
loss = model_3.evaluate(X_test , y_test)

# Predictions on training and validation data
y_pred_3_train = model_3.predict(X_train1)
y_pred_3_val = model_3.predict(X_val)

# Calculate R2 score for training and validation data
rscore_train = r2_score(y_train1, y_pred_3_train)
rscore_val = r2_score(y_val, y_pred_3_val)

# Calculate R2 score for testing data
y_pred_3_test = model_3.predict(X_test)
rscore_3 = r2_score(y_test , y_pred_3_test)

print(f"Test R2score: {rscore_3}")
print(f"Train R2score: {rscore_train}")
print(f"Validation R2score: {rscore_val}")

# Plot the training and validation loss
plt.plot(history.history['loss'], label='train')   # Training loss
plt.plot(history.history['val_loss'], label='val')  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Trainng and Validation Loss functions")
plt.show()

"""# Part 7

## SGD + MAE
"""

# Compile model with stochastic gradient descent optimizer and mean absolute error loss
model_3.compile(optimizer = 'sgd',loss = 'mae')

# Split the data into training and validation sets
X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=93, shuffle=True)

history = model_3.fit(X_train1, y_train1, validation_data=(X_val, y_val), epochs=100, batch_size=10, verbose=0)

# Evaluate the model
loss = model_3.evaluate(X_test , y_test)

# Predictions on training and validation data
y_pred_3_train = model_3.predict(X_train1)
y_pred_3_val = model_3.predict(X_val)

# Calculate R2 score for training and validation data
rscore_train = r2_score(y_train1, y_pred_3_train)
rscore_val = r2_score(y_val, y_pred_3_val)

# Calculate R2 score for testing data
y_pred_3_test = model_3.predict(X_test)
rscore_3 = r2_score(y_test , y_pred_3_test)

print(f"Test R2score: {rscore_3}")
print(f"Train R2score: {rscore_train}")
print(f"Validation R2score: {rscore_val}")

# Plot the training and validation loss
plt.plot(history.history['loss'], label='train')   # Training loss
plt.plot(history.history['val_loss'], label='val')  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Trainng and Validation Loss functions")
plt.show()

"""# Part 8"""

# Inverse transform the scaled test data and predictions
y_test_unscaled = scaler_2.inverse_transform(y_test)
y_pred_unscaled = scaler_2.inverse_transform(y_pred_3_test)

random_pred = []
random_test = []

for i in range(5):
  j = random.randint(0, len(y_test_unscaled))
  random_pred.append(y_pred_unscaled[i])
  random_test.append(y_test_unscaled[i])

# Plotting the unscaled true test data against predictions with different colors
plt.figure(figsize=(8, 6))
plt.scatter(random_test, random_pred, color='red', label='Predicted')
plt.scatter(random_test, random_test, color='blue', label='True')
plt.title('True vs Predicted Values for 5 random data in Test data')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.legend()
plt.show()

# Assuming random_test and random_pred are lists
random_test = np.array(random_test)
random_pred = np.array(random_pred)

# Calculate errors between true and predicted values
errors = np.abs(random_test - random_pred)

# Plotting the errors
plt.figure(figsize=(8, 6))
plt.plot(errors, marker='o', linestyle='', color='green')
plt.title('Absolute Errors between True and Predicted Values for 5 random data in Test data')
plt.xlabel('Sample')
plt.ylabel('Absolute Error')
plt.show()

print("True values:")
print(random_test)
print("\nPredicted values:")
print(random_pred)

"""## Improving Model

### MLP with 3 hidden layer
"""

model_3 = Sequential()

# Add a hidden layer with 10 neurons, ReLU activation function
model_3.add(Dense(10, activation='relu', input_shape=(X_train.shape[1],)))

# Add the second hidden layer with 10 neurons, ReLU activation function
model_3.add(Dense(10, activation='relu', input_shape=(X_train.shape[1],)))

# Add the third hidden layer with 10 neurons, ReLU activation function
model_3.add(Dense(10, activation='relu', input_shape=(X_train.shape[1],)))

# Add an output layer with 1 neuron and linear activation function
model_3.add(Dense(1, activation='linear'))

model_3.summary()

from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf

# Model compilation
model_3.compile(optimizer='sgd', loss='mae')

# Define a checkpoint to save the best model during training
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

# Training with callbacks
history = model_3.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=10, callbacks=[checkpoint])
# callbacks=[reduce_lr, early_stop, lr_scheduler, checkpoint]
# Evaluate the model
loss = model_3.evaluate(X_test , y_test)

# Predictions on training and validation data
y_pred_3_train = model_3.predict(X_train)
y_pred_3_val = model_3.predict(X_val)

# Calculate R2 score for training and validation data
rscore_train = r2_score(y_train, y_pred_3_train)
rscore_val = r2_score(y_val, y_pred_3_val)

# Calculate R2 score for testing data
y_pred_3_test = model_3.predict(X_test)
rscore_3 = r2_score(y_test , y_pred_3_test)

print(f"Test R2score: {rscore_3}")
print(f"Train R2score: {rscore_train}")
print(f"Validation R2score: {rscore_val}")

# Plot the training and validation loss
plt.plot(history.history['loss'], label='train')   # Training loss
plt.plot(history.history['val_loss'], label='val')  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Trainng and Validation Loss functions")
plt.show()