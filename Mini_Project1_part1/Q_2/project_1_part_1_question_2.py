# -*- coding: utf-8 -*-
"""Project_1_Part_1_Question_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OJihYxhQqUGYlF4A6UNTy4xddlBYI-H7

# Part 1
"""

!pip install --upgrade --no-cache-dir gdown
!gdown 11wi71E9PppwbkvUnk0LLPbQ9gprklqLe

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv(r'/content/data_banknote_authentication.txt')
headerlist = ["1'st feature", "2'nd feature", "3'rd feature", "4'th feature", "target"]
data.to_csv('/content/data_banknote_authentication.csv', header=headerlist, index=False)
df = pd.read_csv('/content/data_banknote_authentication.csv')
df

"""# Part 2"""

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

df = shuffle(df, random_state=93)
df

X = df[["1'st feature", "2'nd feature", "3'rd feature", "4'th feature"]].values

y = df[["target"]].values
X ,y

x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.2, random_state=93)
x_train.shape , x_test.shape , y_train.shape , y_test.shape

"""# Part 3

## Logistic Regression Model

### Sigmoid
"""

def sigmoid(x):
  return 1/(1+np.exp(-x))

def logistic_regression(x, w):
  y_hat = sigmoid(x @ w)
  return y_hat

"""### Binary Cross Entropy(BCE)"""

def bce(y, y_hat):
  loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
  return loss

"""### Gradient"""

def gradient(x, y, y_hat):
  grads = (x.T @ (y_hat-y))/len(y)
  return grads

"""### Gradient Descent"""

def gradient_descent(w, eta, grads):
  w -= eta*grads
  return w

"""### Accuracy"""

def accuracy(y, y_hat):
  acc = np.sum(y == np.round(y_hat))/len(y)
  return acc

"""### Train"""

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))
x_train.shape

m = 4 # number of features
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01 # learning rate
n_epochs = 4500 # number of epochs

error_hist = []

for epoch in range(n_epochs):
  # predictions
  y_hat = logistic_regression(x_train, w)

  # loss
  e = bce(y_train, y_hat)
  error_hist.append(e)

  # gradients
  grads = gradient(x_train, y_train, y_hat)

  # gradient descent
  w = gradient_descent(w, eta, grads)

  if (epoch+1) % 100 ==0:
    print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

"""### Loss fuction"""

plt.figure(figsize=(8, 6))
plt.plot(error_hist)
plt.xlabel("Number of Epochs")
plt.ylabel("Error")
plt.title('Loss Function')
plt.grid(alpha=0.5)

"""### Test"""

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test, w)
accuracy(y_test, y_hat)

"""# Part 4"""

mean = np.mean(X, axis=0)
std = np.std(X, axis=0)

normalized_X = (X - mean) / std
normalized_X

"""# Part 5"""

from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(normalized_X , y , test_size = 0.2, random_state=93)
x_train.shape , x_test.shape , y_train.shape , y_test.shape

"""## Logistic Regression Model

### Sigmoid
"""

def sigmoid(x):
  return 1/(1+np.exp(-x))

def logistic_regression(x, w):
  y_hat = sigmoid(x @ w)
  return y_hat

"""### Binary Cross Entropy(BCE)"""

def bce(y, y_hat):
  loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
  return loss

"""### Gradient"""

def gradient(x, y, y_hat):
  grads = (x.T @ (y_hat-y))/len(y)
  return grads

"""### Gradient Descent"""

def gradient_descent(w, eta, grads):
  w -= eta*grads
  return w

"""### Accuracy"""

def accuracy(y, y_hat):
  acc = np.sum(y == np.round(y_hat))/len(y)
  return acc

"""### Train"""

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))
x_train.shape

m = 4 # number of features
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01 # learning rate
n_epochs = 4500 # number of epochs

error_hist = []

for epoch in range(n_epochs):
  # predictions
  y_hat = logistic_regression(x_train, w)

  # loss
  e = bce(y_train, y_hat)
  error_hist.append(e)

  # gradients
  grads = gradient(x_train, y_train, y_hat)

  # gradient descent
  w = gradient_descent(w, eta, grads)

  if (epoch+1) % 100 ==0:
    print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

"""### Loss fuction"""

plt.figure(figsize=(8, 6))
plt.plot(error_hist)
plt.xlabel("Number of Epochs")
plt.ylabel("Error")
plt.title('Loss Function')
plt.grid(alpha=0.5)

"""### Test"""

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test, w)
accuracy(y_test, y_hat)

# Number of desired samples
num_samples = 5
# Select 5 random indices from the test data
random_indices = np.random.choice(len(x_test), num_samples, replace=False)

# Data samples corresponding to the selected indices
random_samples = x_test[random_indices]
y_random_samples = y_test[random_indices]

y_hat_random_samples = logistic_regression(random_samples, w)

# Display the model predictions and real targets for the selected samples
print("Model Predictions for the Selected Samples:")
print(y_hat_random_samples)

print("Real Targets for the Selected Samples:")
print(y_random_samples)

"""# Part 6"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

new_y = pd.DataFrame(y)
new_y.value_counts()

plt.figure(figsize=(8, 6))
new_y.value_counts().plot.pie(autopct = "%.2f")
plt.title("the distribution of samples in each class")
plt.legend(labels=new_y.value_counts().index, title="Class Labels", loc="upper right")

! pip install -U imbalanced-learn

from imblearn.under_sampling import RandomUnderSampler

y1 = pd.DataFrame(y)
rus = RandomUnderSampler(sampling_strategy=1, random_state=93)
X_res_undersampling , y_res_undersampling = rus.fit_resample(X , y1)
X_res_undersampling.shape , y_res_undersampling.shape

y_res_undersampling.value_counts()

plt.figure(figsize=(8, 6))
y_res_undersampling.value_counts().plot.pie(autopct = '%.2f')
plt.title("the distribution of samples in each class afetr undersampling")
plt.legend(labels=new_y.value_counts().index, title="Class Labels", loc="upper right")

"""# Part 7"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_res_undersampling, y_res_undersampling, test_size=0.2, random_state=93, stratify=y_res_undersampling)

"""## Logistic Regression"""

LogReg_classifier = LogisticRegression(C=1.0, penalty='l2', solver='sag', max_iter=20000, random_state=93)
LogReg_classifier.fit(X_train, y_train)
y_pred = LogReg_classifier.predict(X_test)
LogReg_train_accuracy = LogReg_classifier.score(X_train, y_train)
LogReg_test_accuracy = LogReg_classifier.score(X_test, y_test)

"""## Accuracy"""

print(f"Logistic Regression Train Accuracy: {LogReg_train_accuracy:.2f}")
print(f"Logistic Regression Test Accuracy: {LogReg_test_accuracy:.2f}")